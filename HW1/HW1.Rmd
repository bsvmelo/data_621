---
title: "DATA 621 Homework 1"
subtitle: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document: 
    toc: true
    toc_float: true
  pdf_document: default
  html_notebook: default
---

## Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.

## Objective 

The objective is to build a multiple linear regression model on the training data to predict the number of wins for the team. we can only use the variables given to us (or variables that we derive from the variables provided). 

## 1. Data Exploration


### Data Summery 

```{r, echo=FALSE}
#Import required libraries
library(tidyr)
library(zoo)
library(pastecs)
library(dplyr)
library(ggplot2)
library(corrr)
library(GGally)
library(corrplot)
library(ggcorrplot)
library(reshape2)
```


```{r}
#Import the data
Data <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW1/moneyball-training-data.csv")
head(Data)
```

We can see that the data contain 17 columns and 2276 observations or records. The first column is the index which will be deleted as it is not useful. 

```{r}
#Remove the index
Data1 <- Data [-c(1)]

```


```{r}
#Check the Summary
summary(Data1)
```
Summary of the data gives a useful information about each feature including the number of NA values. It is obvious that we have many NA values. 

```{r}
# Compute descriptive statistics
res <- stat.desc(Data1)
round(res,2)
```

```{r}
#The mean for each column in the data
colMeans(Data1)
```

```{r}
#The Standard Deviation for each column in the data
sapply(Data1, sd)
```

```{r}
#The median for each column in the data
apply(Data1,2, median)
```

### Missing Vlaues

```{r}
#Search if there are any NA values

sum(is.na(Data1))
```

```{r}
#We are not able to delete the NA values. We will replace NA values.

Data2 = replace(Data1, TRUE, lapply(Data1, na.aggregate))
```

```{r}
#Confirm the all NA values were replaced by the mean.
sum(is.na(Data2))
```


```{r}
#Confirm that data is numeric
sapply(Data2, is.numeric)
```


### Graphs

```{r}
hist(Data2$TARGET_WINS)
```

The histogram of the target_wins column is normally distributed. 

```{r}
Data2 %>%
  gather(var, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_density(color = "blue") + 
  facet_wrap(~var, scales= "free", ncol = 4)
         
```


```{r}
Data2 %>%
  gather(var, value, TARGET_WINS:TEAM_FIELDING_DP) %>%
  ggplot(., aes(value)) + 
  geom_boxplot(notch = TRUE) + 
  facet_wrap(~var, scales= "free", ncol = 4)
```


### Correlation

```{r}
# Use pearson correlation
corrr:: correlate (Data2, method = "pearson")

```



```{r}
ggcorr(Data2)

```



```{r}
#Add correlation coefficients
corr <- round(cor(Data2), 1)
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   lab = TRUE)
```




# Data Preperation
In this section we will be looking at the different ways to prepare the data for modeling. We will show the different steps that we took and the reasoning why we did certain transformations, replacement and creation of columns.

```{r importing data set}
moneyball_training_data = read.csv("moneyball-training-data.csv")
```


```{r finding all NA}
na_count = sapply(moneyball_training_data, function(y) sum(is.na(y)))
na_count = data.frame(na_count)
na_count %>%
  arrange(desc(na_count)) %>%
  mutate(total_rows = nrow(moneyball_training_data)) %>%
  mutate(percent_missing = na_count / total_rows)
```
Initially when looking at the data we can see that **TEAM_BATTING_HBP** is missing 91% of its data and **TEAM_BASERUN_CS** is missing around 34% of its data. This is a lot of data missing which is why those columns will be removing these. Based on online reading there is no definite cut of for how much data one should be missing before removing a column, but it is always better to have more data. The columns **TEAM_FIELDING_DP**, **TEAM_BASERUN_SB**, **TEAM_BATTING_SO**, and **TEAM_PITCHING_SO** are missing around 12% - 4% of its data and can fill those in with using mean and median. In the next section we will look at to see whether using the mean or median would be the better choice in filling the missing data.

```{r removing column}
moneyball_subset = subset(moneyball_training_data, select=-c(TEAM_BATTING_HBP, TEAM_BASERUN_CS, INDEX))
```


## Replacing NA with Mean or Median

In this section we will need to decide whether to fill the missing data using the mean or median. We will need to look at the distribution of each of the columns with missing data in order to decide if we will be using the median or mean to fill in the missing data

```{r}
missing_data = subset(moneyball_subset, select = c(TEAM_FIELDING_DP, TEAM_BASERUN_SB, TEAM_BATTING_SO, TEAM_PITCHING_SO))
missing_data = melt(missing_data)

ggplot(missing_data, aes(x = value)) + geom_histogram(binwidth = 10) + facet_wrap(~variable, scale='free')
```
Looking at the above graphs we can see that not all the distribution are uniform distribution. We can see that **TEAM_BATTING_SO** is a bimodal distribution, **TEAM_BASERUN_SB** is skewed to the right, and **TEAM_PITCHING_SO** has very large outliers. For this reason we will be using the median to replace all the missing data as the median is less susceptible to outliers and non-uniform distributions.

```{r}
replace_na_with_median = function(x){
  x[is.na(x)] = median(x, na.rm=TRUE)
  return(x)
}

moneyball_fill = apply(moneyball_subset, 2, replace_na_with_median)
moneyball_fill = as.data.frame(moneyball_fill)
```



## Transformation
We will also be needing to check all of the columns to see if they will need any type of transformation in order to create a linear line. We will be be graphing all the columns with **TARGET_WINS** as the response variable. This will allow us to see if there are any columns that can be transformed in order to improve the model.

```{r}
par(mfrow=c(2,2))


for (i in 2:ncol(moneyball_fill)){
  
  y = moneyball_subset[,1]
  x = moneyball_subset[,i]
  plot(
    x, 
    y,   
    ylab = 'TARGET_WINS',
    xlab = names(moneyball_fill)[i]
  )
}
```
Looking at the graphs above we can see that none of the columns are real good candidates for transformation.


## Putting Teams Into Buckets

We will be putting the dataset into buckets based on the teams winning score as this will allow us to see if there is any patterns between weak and strong teams. The teams will be split into two groups **Strong** and **Weak** based on the **TARGET_WINS** column. 

```{r}
moneyball_fill$TEAM_TYPE = cut(moneyball_subset[,'TARGET_WINS'], breaks=c(0, 73, 146), include.lowest = TRUE, labels = c('Weak', 'Strong'))
```


## Creating Total Hits
Creating a column which includes the total amount of hits a team has

```{r}
moneyball_fill$TEAM_BATTING_TOTAL = (moneyball_fill$TEAM_BATTING_H + (2 * moneyball_fill$TEAM_BATTING_2B) + (3 * moneyball_fill$TEAM_BATTING_3B) + (4 * moneyball_fill$TEAM_BATTING_HR))
```

```{r}
ggplot(moneyball_fill, aes(x=TEAM_BATTING_TOTAL, y=TARGET_WINS)) + geom_smooth(method='lm') + geom_point(aes(color=TEAM_TYPE))
```

## Hit Percentage
We would like to create a column which states what is the teams hit/base they get per game. This will be calculated by summing the total amount of hits a team gets and dividing 162 game season.

```{r}
moneyball_fill$TEAM_BATTING_PERCENT =  moneyball_fill$TEAM_BATTING_TOTAL / 162
```

```{r}
ggplot(moneyball_fill, aes(x=TEAM_BATTING_PERCENT, y=TARGET_WINS)) + geom_smooth(method='lm') + geom_point(aes(color=TEAM_TYPE))
```
## Model Building

At the beginning, we were presented with 16 independent variables. It makes sense to exclude index since it is not relevant. It also makes sense to exclude team_batting_hbp and team_batting_cs since they are comprised of so many N/As. We are thus able to concentrate on the 13 remaining variables, pursuing continuous incremental model improvement.

Our models are outlined below:

  lmodel1 - an "all-in" model that includes all 13 remaining variables
  lmodel2 - a model that strips out outliers
  lmodel3 - a model that eliminates impertinent attributes
  
```{r}

names(moneyball_fill) <- tolower(names(moneyball_fill))
#let's strip out the team type since it doesn't enhance the model
train1 <- subset(moneyball_fill, select = -c(team_type))
head(train1)
```

We'll start with the all-in model
```{r}

lmodel1 <- lm(target_wins ~ ., data = train1)
summary(lmodel1)
```
So in looking at the all-in model, we can identify how the model behaves intuitively and not-so-intuitively. For example, we see the following variables as having positive coefficients: team_batting_h, team_batting_3b, team_baserun_sb, and team_pitching_strikeouts. These make sense, as you'd expect a team to win games that gets hits, hits triples, steals bases efficiently, and strikes out opponents. However, some of the positive coefficients don't make as much sense. For example, we would expect teams whose pitchers give up lots of home runs to not win very many games. This certainly warrants further analysis.

For negative coefficients, we'd obviously expect teams whose players make a lot of errors to not win at a high rate. However, hitting doubles and fielding double plays have negative coefficients as well, which are not intuitive at all.

A majority of the variables that we are assessing appear to contribute to predicting wins. We can gain some comfort in our model due to the low RSE (13.07) and satisfactory F-statistic (80.1), and we should feel ok about the overall efficacy of our model. However, the Adjusted R-square well under 1 is cause for some concern, but we can look to improve that in future iterations of the model. 

What else can we do to improve our model? Well, its predictive value might be enhanced by eliminating some problematic outliers. So let's take a look at if it makes sense to do so.
```{r}
res1 <- resid(lmodel1)
plot(fitted(lmodel1), res1)

abline(0,0)

qqnorm(res1, pch = 1, frame = FALSE)
```
The data is not evenly scattered but we don't detect any unexpected non-linear patter. The normal QQ looks good as well with a relatively straight line. We can spot some outliers that we should drill down on using Cook's Distance. Then, we can then attempt to strip them out to improve our model somewhat.

```{r}
cooksd <- cooks.distance(lmodel1)
sample_size <- nrow(train1)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4/sample_size, col="red")  # add cutoff line
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")
```
We can spot two that breach our threshold, so now we set about removing them. Next, we can re-run our initial all-in model to see if dropping the outliers has any impact on improving the model.

```{r}
influential <- as.numeric(names(cooksd)[(cooksd > (4/sample_size))])
train1_strip <- train1[-influential, ]

lmodel2 <- lm(target_wins ~ ., data = train1_strip)
summary(lmodel2)
```
This looks like good news. Our RSE is down, and our F-statistic is up. Even our Adjusted R-Squared value is up slightly from .31. Nevertheless, the explanatory value of our model remains limited without this last number increasing significantly. And we can clearly see some variables with high p-values that ought to be removed in order to improve our model. Let's proceed with removing team_batting_hr and team_picthing_hr.

```{r}

train3 <- subset(train1_strip, select = -c(team_batting_hr,team_pitching_hr))
lmodel3 <- lm(target_wins ~ ., data = train3)
summary(lmodel3)
```

We've improved the model incrementally by removing variables with high p-values, and our RSE and F-stat look better The explanatory power of our model, however, remains in doubt due to the Adjusted R-Squared value that remains low, even though it's improved from the previous model. What stands out here is that triples hit, bases stolen, and gaining walks remain the overall strongest positive coefficients, while team_fielding_dp remains the largest negative coefficient, which is counter-intuitive at first blush. However, one thing necessary for a double play is at least one opponent runner on base. Those teams that earn a high number of double plays are only able to do so because their pitchers are allowing runners on base to begin with. 

