---
title: "DATA 621 Homework 5"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 – Business Analytics and Data Mining\\
\medskip
Home Work 5\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{r message=FALSE, warning=FALSE, echo=FALSE}
if (!require('ggplot2')) (install.packages('ggplot2'))
if(!require('dplyr')) (install.packages('dplyr'))
if(!require('tidyverse')) (install.packages('tidyverse'))
if(!require('purrr')) (install.packages('purrr'))
if(!require('mice')) (install.packages('mice'))
if(!require('DataExplorer')) (install.packages('DataExplorer'))
if(!require('MASS')) (install.packages('MASS'))
if(!require('caret')) (install.packages('caret'))
if(!require('stats')) (install.packages('stats'))
if(!require('pROC')) (install.packages('pROC'))
if(!require('kableExtra')) (install.packages('kableExtra'))
if(!require('gridExtra')) (install.packages('gridExtra'))
if(!require('ggcorrplot')) (install.packages("ggcorrplot"))
if(!require('fitdistrplus')) (install.packages("fitdistrplus"))
if (!require('pacman')) (install.packages('pacman'))
if (!require('GGally')) (install.packages('GGally'))
if(!require('randomForest')) (install.packages('randomForest'))
library(corrplot)
library(RColorBrewer)

# install.packages('mice')
# library(mice)

pacman::p_load('ggplot2', 'reporttools', 'dplyr', 'MASS', 'dplyr', 'psych', 'DataExplorer', 'mice', 'pscl', 'pander', 'tinytex', 'ggcorrplot')
```


\newpage

# Introduction

```{r, message=F, warning=FALSE, echo=FALSE}
theme_update(plot.title = element_text(hjust = 0.5), 
             axis.text.x = element_text(angle = 90, hjust = 1))

train_data <- read.csv('https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW5/wine-training-data.csv', header=T)
eval_data <- read.csv('https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW5/wine-evaluation-data.csv', header=T)
```

## Problem

Our goal is to explore, analyze and model a dataset containing information on approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine distribution companies after sampling a wine.  These cases would be used to provide tasting samples to restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a wine to be sold at a high end restaurant.

A large wine manufacturer is studying the data in order to predict  the number of wine cases ordered based upon the wine characteristics. If the wine manufacturer can predict the number of cases, then that manufacturer will be able to adjust their wine offering to maximize sales.

The objective is to build a count regression model to predict the number of cases of wine that will be sold given certain properties of the wine.

\newpage

# Data Exploration  

Below we'll display a few basic EDA techniques to gain insight into our wine dataset.

## Basic Statistics

The data is 1.3 Mb in size. There are 12,795 rows and 15 columns (features). Of all 15 columns, 0 are discrete, 15 are continuous, and 0 are all missing. There are 8,200 missing values out of 191,925 data points.

```{r, echo=FALSE, warning=FALSE}
summary <- describe(train_data[,c(1:15)])[,c(2:5,8,9,11,12)]
#knitr::kable(summary)
print(summary)
```
It's useful to note a couple of things right off the bat with regard to our dataset:
- There are several variables that have negative values.
- ResidualSugar, Chlorides, FreeSulfurDioxide, and TotalSulfurDioxide all have quite a few missing values that we are going to need to deal with in order to assess the variables.
- The Index column is useless and can be ignored.

## Histogram of Variables

```{r, echo=FALSE, warning=FALSE}
plot_histogram(train_data)
```

Based on the histograms we can see that a lot of the variables distributions looks to be a normal distribution. We can see that **AcidIndex**, **STARS**, and **TARGET** are a bit skewed. One thing to note is that the **TARGET** variable has a lot of 0 cases sold. These 0 **TARGET** variables will need to be cleaned during the data prep phase as they can skew the results of the model.

## Relationship of Predictors to Target
It is useful to assess the plots of each variable against the target variable. Using the GGPairs function from GGally we can plot someof the variables of interest to see if any of the variables correlates with the response variable **TARGET**. We will be making sure to include the variables **STARS** and **LabelAppeal** as it is believed that these two variables affect sales numbers

```{r, echo=FALSE, warning=FALSE}
# plot_scatterplot(train_data[2:15,], "TARGET")
# featurePlot(train_data[,2:ncol(train_data)], train_data[,1], pch = 20)
train_data %>%
  dplyr::select(TARGET, STARS, LabelAppeal, Alcohol) %>%
  ggpairs()
```

We can see that **STARS** and **Alcohol** has a bit of correlation with **TARGET**.

## Boxplots
After observing our distributions, we can next assess the variables' relationship with our target variable (TARGET).

```{r, echo=FALSE, warning=FALSE}
bp_train <- train_data %>% 
  gather(key = 'variable', value = 'value')

ggplot(bp_train, aes(variable, value)) + 
  geom_boxplot() + 
  facet_wrap(. ~variable, scales='free', ncol=4)+ coord_flip()
  
```

When looking at the boxplot for **TARGET** we can see a very different picture compared to looking at the histogram. In the histogram it shows all the 0 **TARGETS** which can skew the modeling results while in the histogram one can not easily point that out. This is the reason why we need to look at the data in mulitple different ways.

# Data Preparation

## Identify Missing Values

We can see that the same variables for both train data set and evaluation data set contains missing values. The variable that contains the most missing values is the **STAR** followed by **Sulphates**, **Alcohol**, **ResidualSugar**, **TotalSulfurDioxide**, **FreeSulfurDioxide**, **Chlorides**, and **pH**.

```{r, echo=FALSE, warning=FALSE}
# Missing values in train data set
colSums(is.na(train_data[-c(1)])) %>% data.frame() %>% `colnames<-`("Missing Count") %>% tibble::rownames_to_column("Train Data Set Variales") 
plot_missing(train_data[-c(1)])

# Missing values in evaluation data set
colSums(is.na(eval_data[-c(1)])) %>% data.frame() %>% `colnames<-`("Missing Count") %>% tibble::rownames_to_column("Train Data Set Variales")
plot_missing(eval_data[-c(1,2)])
```
Our chart above does a good job of highlighting the missing values that will no doubt impact our analysis if we don't deal with them. In particular:

-   STARS is missing 25% of its records. We could guess that the wines haven't been assessed and rated by experts. 
-   Sulphates is missing 9% of its records. 
-   Alcohol is missing 5% of its records. It is unlikely that a 0 here would indicate no alcohol in the wine, given that it's wine, so we can assume these values are missing. 
-   ResidualSugar is missing 5% of its records. There are some records that have 0 for ResidualSugar so these flagged records most certainly have missing values.
-   A few more variables experiences missing values as well such as TotalSulfurDioxide (<5%), FreeSulfurDioxide (<5%), Chlorides (<5%), and pH (<4%).

## Impute Missing Values

We assume the missing data are Missing at Random and choose to impute. The reason we want to impute the missing data rather than replacing with mean or median because of large number of missing values. If we're replacing with mean or median on the large number of missing values, can result in loss of variation in data. We're imputing the missing data using the MICE package. The method of predictive mean matching (PMM) is selected for continuous variables.

```{r, echo=FALSE, warning=FALSE}
# Impute both train and evaluation data set
impute_train_data <- mice(train_data[-c(1)], m=5, maxit=20, method='pmm', seed=321, print = FALSE)
densityplot(impute_train_data) 
impute_eval_data <- mice(eval_data[-c(1)], m=5, maxit=20, method='pmm', seed=321, print = FALSE)
densityplot(impute_eval_data)
```

Next, we take average values of the 5 imputed data set as a final train data set used for building models. 

```{r, echo=FALSE, warning=FALSE}
# Combine all 5 imputed data set and calculate the average value
complete_train_data <- (complete(impute_train_data,1) + complete(impute_train_data,2) + complete(impute_train_data,3) + complete(impute_train_data,4) + complete(impute_train_data,5))/5

complete_eval_data <- (complete(impute_eval_data,1) + complete(impute_eval_data,2) + complete(impute_eval_data,3) + complete(impute_eval_data,4) + complete(impute_eval_data,5))/5

# Box plot for each variable
complete_train_data %>% gather(variable, value) %>% 
  ggplot(aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=variable)) + facet_wrap( ~ variable, scales="free") + 
  theme(legend.position = "none", axis.text.y=element_blank()) + coord_flip()

# Plot density plot for each variable
complete_train_data %>% plot_density()

```

Because we'll use the Poisson or Negative Binomial regression to build count regression models in GLM approach, transformation for each variable to make them looks normal is not required. Diagnostics of actual outliers or influential points can be identified in the build models section through plots such as residuals vs fitted, standardized residuals vs fitted, etc.

## Identifying Multicollinearity
```{r, echo=FALSE, warning=FALSE}
stack(sort(cor(complete_train_data[,1], complete_train_data[,2:ncol(complete_train_data)])[,], decreasing=TRUE))
correlation = cor(complete_train_data, use = 'pairwise.complete.obs')
corrplot(correlation, method='ellipse', type = 'lower', order = 'hclust',col=brewer.pal(n=8, name="RdYlBu"))
```

After our EDA and Data Prep, we can render some judgments on the dataset, including that there are some variables with a weak enough relationship with our TARGET that we can plan to drop them. We also need to plan on dealing with the many outliers that could skew our models and pay close attention to multicollinearity, especially as it relates to the relationship between STARS and LabelAppeal. These two features also happen to have the strongest correlation with the TARGET.

# Build Models

## Poisson Model 1
We start with a quasi-Poisson model using all variables.
```{r, echo=TRUE, warning=FALSE}
Model1 <- glm(TARGET ~ ., data=complete_train_data,family=quasipoisson)
summary(Model1)

```

We see that the dispersion parameter is close to 1, meaning this regression is close to a regular Poisson regression case.


Using the F test, we check the significance of each of the predictors relative to the full model:

```{r, echo=TRUE, warning=FALSE}
drop1(Model1, test="F")
```


Non significant predictors will be dropped and a reduced model is estimated below.

## Poisson Model 2
This is the model with only the significant variables at the 5% level.

```{r, echo=TRUE, warning=FALSE}
Model2 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + pH + Sulphates+ LabelAppeal+
                AcidIndex + STARS,data=complete_train_data, family=quasipoisson)
summary(Model2)
```
We see there is little practical difference between the two models.


## Negative binomial regression 

We also estimate a regression using a negative binomial regression.

```{r, echo=TRUE, warning=FALSE}
Model3 <- glm.nb(TARGET ~ ., complete_train_data)
summary(Model3)
```

## Zero Inflated Count Models
Considering that the response data has a lot of zeros, we will also estimate a regression using zero inflated count models.

```{r, echo=TRUE, warning=FALSE}
Model4<- zeroinfl(TARGET ~ ., data = complete_train_data)
summary(Model4)
```

As per our text book, we will also estimate a simplified version considering two components: non-count and count variables.


```{r, echo=TRUE, warning=FALSE}
Model5 <- zeroinfl(TARGET ~ .|STARS, data = complete_train_data)
summary(Model5)
```




# Model Selection

In order to select a model, we will compare various metrics for all five models using the training data set. We check models’ confusion matrix and its measures such as accuracy, classification error rate, precision, sensitivity, specificity, F1 score, AUC, and also MAE, RMSE, and R-squared.


```{r echo=FALSE, message=FALSE, results=FALSE}
# comparing all models using various measures

pred1<-fitted(Model1)
pred1<-ifelse(pred1>=8,8,pred1)
CM1 <- confusionMatrix(as.factor(as.integer(pred1)), as.factor(Model1$y))


pred2<-fitted(Model2)
pred2<-ifelse(pred2>=8,8,pred2)
CM2 <- confusionMatrix(as.factor(as.integer(pred2)), as.factor(Model2$y))

pred3<-fitted(Model3)
pred3<-ifelse(pred3>=8,8,pred3)
CM3 <- confusionMatrix(as.factor(as.integer(pred3)), as.factor(Model3$y))

CM4 <- confusionMatrix(as.factor(as.integer(fitted(Model4))), as.factor(Model4$y))

CM5 <- confusionMatrix(as.factor(as.integer(fitted(Model5))), as.factor(Model5$y))

# CM6 <- confusionMatrix(as.factor(as.integer(fitted(Model6))), as.factor(Model6$y))

# pred6<-predict(Model6, complete_train_data,type="response")


Roc1 <- multiclass.roc(complete_train_data$TARGET,  predict(Model1, complete_train_data, interval = "prediction"), 
levels=base::levels(as.factor(complete_train_data$TARGET)))

Roc2 <- multiclass.roc(complete_train_data$TARGET,  predict(Model2, complete_train_data, interval = "prediction"), 
levels=base::levels(as.factor(complete_train_data$TARGET)))

Roc3 <- multiclass.roc(complete_train_data$TARGET,  predict(Model3, complete_train_data, interval = "prediction"), 
levels=base::levels(as.factor(complete_train_data$TARGET)))

Roc4 <- multiclass.roc(complete_train_data$TARGET,  predict(Model4, complete_train_data, interval = "prediction"), 
levels=base::levels(as.factor(complete_train_data$TARGET)))

Roc5 <- multiclass.roc(complete_train_data$TARGET,  predict(Model5, complete_train_data, interval = "prediction"), 
levels=base::levels(as.factor(complete_train_data$TARGET)))

# Roc6 <- multiclass.roc(complete_train_data$TARGET,  predict(Model6, complete_train_data, interval = "prediction"), 
# levels=base::levels(as.factor(complete_train_data$TARGET)))


metrics1 <- c(CM1$overall[1], "Class. Error Rate" = 1 - as.numeric(CM1$overall[1]), "Sensitivity" = CM1$byClass[1], "Specificity" = CM1$byClass[2], "Precision" =  CM1$byClass[5] , "F1" =   CM1$byClass[7], AUC = Roc1$auc, "MAE" = MAE(fitted(Model1),Model1$y), "RMSE" = RMSE(fitted(Model1),Model1$y), "R2" = R2(fitted(Model1),Model1$y))

metrics2 <- c(CM2$overall[1], "Class. Error Rate" = 1 - as.numeric(CM2$overall[1]), "Sensitivity" = CM2$byClass[1], "Specificity" = CM2$byClass[2], "Precision" =  CM2$byClass[5] , "F1" =   CM2$byClass[7], AUC = Roc2$auc, "MAE" = MAE(fitted(Model2),Model2$y), "RMSE" = RMSE(fitted(Model2),Model2$y), "R2" = R2(fitted(Model2),Model2$y))

metrics3 <- c(CM3$overall[1], "Class. Error Rate" = 1 - as.numeric(CM3$overall[1]), "Sensitivity" = CM3$byClass[1], "Specificity" = CM3$byClass[2], "Precision" =  CM3$byClass[5] , "F1" =   CM3$byClass[7], AUC = Roc3$auc, "MAE" = MAE(fitted(Model3),Model3$y), "RMSE" = RMSE(fitted(Model3),Model3$y), "R2" = R2(fitted(Model3),Model3$y))

metrics4 <- c(CM4$overall[1], "Class. Error Rate" = 1 - as.numeric(CM4$overall[1]), "Sensitivity" = CM4$byClass[1], "Specificity" = CM4$byClass[2], "Precision" =  CM4$byClass[5] , "F1" =   CM4$byClass[7], AUC = Roc4$auc, "MAE" = MAE(fitted(Model4),Model4$y), "RMSE" = RMSE(fitted(Model4),Model4$y), "R2" = R2(fitted(Model4),Model4$y))

metrics5 <- c(CM5$overall[1], "Class. Error Rate" = 1 - as.numeric(CM5$overall[1]), "Sensitivity" = CM5$byClass[1], "Specificity" = CM5$byClass[2], "Precision" =  CM5$byClass[5] , "F1" =   CM5$byClass[7], AUC = Roc5$auc, "MAE" = MAE(fitted(Model5),Model5$y), "RMSE" = RMSE(fitted(Model5),Model5$y), "R2" = R2(fitted(Model5),Model5$y))


```



```{r metrics table, echo=FALSE, eval=TRUE}
kable(cbind(metrics1, metrics2, metrics3, metrics4, metrics5), col.names = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5"))  %>% 
  kable_styling(full_width = T)
```


In the table above, we see that a our Zero Inflated Count Models have the highest accuracy, R-squared and lowest MAE and RMSE. We'll make our predictions using the Zero Inflated Count - Model4.



## Make Predictions

We show below a table of the fitted values, using the predictions based on Model4:


```{r, echo=FALSE, warning=FALSE}
prediction<- predict(Model4, complete_eval_data, type="response")
complete_eval_data$TARGET <- prediction
print(table(round(Model4$fitted.values)))
write.csv(prediction, 'HW5preds.csv', row.names = FALSE)

```
And here it is the training set target value distribution:

```{r}
table(complete_train_data$TARGET)
```




# Appendix
* Link to full code : https://github.com/ahussan/DATA_621_Group1/blob/main/HW5/HW5.Rmd

* Link to the predicted values over test set : https://github.com/ahussan/DATA_621_Group1/blob/main/HW5/HW5preds.csv